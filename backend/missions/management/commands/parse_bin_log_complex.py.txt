import os
from pathlib import Path
from django.conf import settings
from datetime import datetime, timezone
from django.core.management.base import BaseCommand, CommandError
from django.utils import timezone as django_timezone
from django.db import transaction
from missions.models import (
    Mission, LogFile, SensorDeployment, 
    NavSample, ImuSample, CompassSample, PressureSample
)
from pymavlink import mavutil
import logging

logger = logging.getLogger(__name__)


class Command(BaseCommand):
    help = "Parse a log file by LogFile ID, unless already parsed."

    def add_arguments(self, parser):
        parser.add_argument(
            "--logfile-id", 
            type=int, 
            required=True, 
            help="ID of the LogFile to parse."
        )
        parser.add_argument(
            "--batch-size", 
            type=int, 
            default=1000, 
            help="Batch size for bulk insert operations."
        )
        parser.add_argument(
            "--force", 
            action="store_true", 
            help="Force re-parsing even if already parsed."
        )

    
    def handle(self, *args, **options):
        logfile_id = options["logfile_id"]
        batch_size = options["batch_size"]
        force = options["force"]
        
        try:
            logfile = LogFile.objects.get(pk=logfile_id)
        except LogFile.DoesNotExist:
            raise CommandError(f"LogFile with id={logfile_id} does not exist.")

        if logfile.already_parsed and not force:
            raise CommandError(f"LogFile {logfile_id} has already been parsed.Use --force to re-parse.")
        
         # Get mission
        mission = logfile.mission
        if not mission.end_time:
            raise CommandError("Mission must have an end_time before import.")
        self.stdout.write(f"Mission found: {mission}")

        # Get deployment IDs for this mission
        deployments = SensorDeployment.objects.filter(mission=mission)
        deployment_ids = list(deployments.values_list('id', flat=True))
        
        self.stdout.write(f"Deployment IDs found: {deployment_ids}")

        # Create deployments lookup by sensor type
        # 'nav' is added as default since we always want that
        deployments_by_type = {}
        for dep in deployments:
            deployments_by_type.setdefault(dep.sensor.sensor_type, ['nav']).append(dep)
        
        self.stdout.write(f"Sensor types found: {list(deployments_by_type.keys())}")
        
        # Initialize the loader
        loader = BinLoader(
            logfile=logfile,
            mission=mission,
            deployments_by_type=deployments_by_type,
            batch_size=batch_size,
            stdout=self.stdout
        )
        
        # Run the parsing
        try:
            loader.run()
            # Mark as parsed
            logfile.already_parsed = True
            logfile.save()
            self.stdout.write(self.style.SUCCESS("Parsing completed successfully."))
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"Parsing failed: {str(e)}"))
            raise

class BinLoader:
    """
    Loader class for parsing ArduPilot .bin log files and extracting sensor data.
    """
    
    # Message type to sensor type mapping
    MESSAGE_TYPE_MAP = {
        "IMU": ("imu", ImuSample),
        "MAG": ("compass", CompassSample),
        "BARO": ("pressure", PressureSample),
        "AHR2": ("nav", NavSample),  # Alternative AHRS message
        "ATT": ("nav", NavSample),   # Attitude message
    }
    
    def __init__(self, logfile, mission, deployments_by_type, batch_size=1000, stdout=None):
        self.logfile = logfile
        self.mission = mission
        self.deployments_by_type = deployments_by_type
        self.batch_size = batch_size
        self.stdout = stdout
        self.stats = {
            "total_messages": 0,
            "filtered_messages": 0,
            "saved_samples": 0,
            "errors": 0
        }
        
        # Validate bin_path exists
        # Combine DATA_DIR and the relative bin_path to get the absolute path
        if not self.logfile.bin_path:
            raise CommandError("No bin_path specified in LogFile.")
        
        bin_path = settings.PROJECT_DIR / self.logfile.bin_path

        if not bin_path.exists():
            raise CommandError(f"Binary log file not found: {bin_path}")
        
        # Initialize pymavlink connection
        try:
            self.log_connection = mavutil.mavlink_connection(
                str(bin_path), 
                dialect="ardupilotmega"
            )
        except Exception as e:
            raise CommandError(f"Failed to open log file: {str(e)}")
    
    def log_message(self, message):
        """Helper to log messages to stdout if available."""
        if self.stdout:
            self.stdout.write(message)
    
    def run(self):
        """Main parsing loop."""
        self.log_message("Starting log file parsing...")
        
        # Initialize batch containers
        batches = {
            ImuSample: [],
            CompassSample: [],
            PressureSample: [],
            NavSample: [],
        }
        
        # Get the log file creation time as baseline for timestamp conversion
        log_file_created = self.logfile.created_at
        
        # Process messages
        for msg in iter(self.log_connection.recv_match, None):
            if msg is None:
                break
                
            self.stats["total_messages"] += 1
            
            try:
                # Convert timestamp to timezone-aware datetime
                timestamp = self._resolve_timestamp(msg, log_file_created)
                
                # Check if timestamp is within mission bounds
                if not (self.mission.start_time <= timestamp <= self.mission.end_time):
                    continue
                
                self.stats["filtered_messages"] += 1
                
                # Get message type and check if we need to process it
                msg_type = msg.get_type()
                if msg_type not in self.MESSAGE_TYPE_MAP:
                    continue
                
                sensor_type, model_class = self.MESSAGE_TYPE_MAP[msg_type]
                
                # Check if we have deployments for this sensor type
                if sensor_type not in self.deployments_by_type:
                    continue
                
                # Create samples for each deployment of this sensor type
                for deployment in self.deployments_by_type[sensor_type]:
                    sample = self._create_sample(msg, timestamp, deployment, model_class)
                    if sample:
                        batches[model_class].append(sample)
                        self.stats["saved_samples"] += 1
                        
                        # Check if we need to flush this batch
                        if len(batches[model_class]) >= self.batch_size:
                            self._flush_batch(model_class, batches[model_class])
                            batches[model_class] = []
                            
            except Exception as e:
                self.stats["errors"] += 1
                logger.error(f"Error processing message {msg.get_type()}: {str(e)}")
                continue
        
        # Flush any remaining batches
        for model_class, batch in batches.items():
            if batch:
                self._flush_batch(model_class, batch)
        
        # Print statistics
        self.log_message(f"Parsing complete! Statistics:")
        self.log_message(f"  Total messages: {self.stats['total_messages']}")
        self.log_message(f"  Filtered messages: {self.stats['filtered_messages']}")
        self.log_message(f"  Saved samples: {self.stats['saved_samples']}")
        self.log_message(f"  Errors: {self.stats['errors']}")
    
    def _resolve_timestamp(self, msg, log_file_created):
        """
        Convert message timestamp to timezone-aware datetime.
        
        Since we don't have GPS data, we'll use the log file creation time
        as a baseline and add the TimeUS (microseconds since boot) offset.
        """
        # Fall back to TimeUS + log creation time
        if hasattr(msg, 'TimeUS') and msg.TimeUS:
            # TimeUS is microseconds since boot
            time_offset_seconds = msg.TimeUS / 1_000_000
            return log_file_created + django_timezone.timedelta(seconds=time_offset_seconds)
        
        # Last resort: use log file creation time
        return log_file_created
    
    def _create_sample(self, msg, timestamp, deployment, model_class):
        """Create a sample object from a message."""
        
        # Common fields for all samples
        sample_data = {
            "log_file": self.logfile,
            "deployment": deployment,
            "timestamp": timestamp,
        }
        
        # Add model-specific fields
        if model_class == ImuSample:
            sample_data.update(self._extract_imu_data(msg))
        elif model_class == CompassSample:
            sample_data.update(self._extract_compass_data(msg))
        elif model_class == PressureSample:
            sample_data.update(self._extract_pressure_data(msg))
        elif model_class == NavSample:
            sample_data.update(self._extract_nav_data(msg))
        else:
            return None
        
        return model_class(**sample_data)
    
    def _extract_imu_data(self, msg):
        """Extract IMU data from message."""
        data = {}
        
        # Gyroscope data (rad/s)
        if hasattr(msg, 'GyrX'): data['gx_rad_s'] = msg.GyrX
        if hasattr(msg, 'GyrY'): data['gy_rad_s'] = msg.GyrY
        if hasattr(msg, 'GyrZ'): data['gz_rad_s'] = msg.GyrZ
        
        # Accelerometer data (m/s²)
        if hasattr(msg, 'AccX'): data['ax_m_s2'] = msg.AccX
        if hasattr(msg, 'AccY'): data['ay_m_s2'] = msg.AccY
        if hasattr(msg, 'AccZ'): data['az_m_s2'] = msg.AccZ
        
        return data
    
    def _extract_compass_data(self, msg):
        """Extract compass/magnetometer data from message."""
        data = {}
        
        # Magnetic field data (µT)
        if hasattr(msg, 'MagX'): data['mx_uT'] = msg.MagX
        if hasattr(msg, 'MagY'): data['my_uT'] = msg.MagY
        if hasattr(msg, 'MagZ'): data['mz_uT'] = msg.MagZ
        
        return data
    
    def _extract_pressure_data(self, msg):
        """Extract pressure/barometer data from message."""
        data = {}
        
        # Pressure data (Pa)
        if hasattr(msg, 'Press'): data['pressure_pa'] = msg.Press
        if hasattr(msg, 'Temp'): data['temperature_C'] = msg.Temp
        
        # Handle alternative pressure message formats
        if hasattr(msg, 'Pressure'): data['pressure_pa'] = msg.Pressure
        if hasattr(msg, 'Temperature'): data['temperature_C'] = msg.Temperature
        
        return data
    
    def _extract_nav_data(self, msg):
        """Extract navigation data from message."""
        data = {}
        
        # Extract depth if available
        if hasattr(msg, 'Alt'): data['depth_m'] = msg.Alt
        if hasattr(msg, 'RelAlt'): data['depth_m'] = msg.RelAlt
        if hasattr(msg, 'PN'): data['depth_m'] = msg.PN  # North position from EKF
        
        # Extract attitude data
        if hasattr(msg, 'Roll'): data['roll_deg'] = msg.Roll
        if hasattr(msg, 'Pitch'): data['pitch_deg'] = msg.Pitch
        if hasattr(msg, 'Yaw'): data['yaw_deg'] = msg.Yaw
        
        return data
    
    def _flush_batch(self, model_class, batch):
        """Flush a batch of samples to the database."""
        if not batch:
            return
        
        try:
            with transaction.atomic():
                model_class.objects.bulk_create(batch, ignore_conflicts=True)
            self.log_message(f"Saved {len(batch)} {model_class.__name__} samples")
        except Exception as e:
            logger.error(f"Error saving batch of {model_class.__name__}: {str(e)}")
            self.stats["errors"] += len(batch)
